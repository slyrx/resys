第一个：
上市公司季度营收的目的就是通过预测营收，来为推荐股票决策做参考。发现一些企业在时间维度上重复出现的特征。在结构上，主要分为离线部分和数据库。在数据方面，是将文件加载到数据库，用离线推荐服务应用 xgboost 和 lstm 融合模型来预测营收。数据的规模是110家上市公司08年到17年的财务数据，包括资产负债表、利润表、现金流量表。

第二个：
句读是一个社交APP，通过每天提供一个好句子，来聚合有共同兴趣的人。项目目标是给用户推荐和用户点赞的话最相似的句子。在框架上，主要是前端、后端、数据库、推荐模块；推荐模块又分为，统计推荐、离线推荐、实时推荐、基于内容的推荐 4 个部分。数据的处理流程是，采集的数据先存入到数据库，

实时处理流程
用户通过接口，做一些网页操作，以服务请求的形式，传到后台。后台会有一个记录，存到日志文件中，经过Flume日志采集之后，利用 kafka 做处理，建立一个数据总线管道，把数据传入到实时计算框架 kafka streaming 里去。streaming 进行一个实时到计算，得到的结果存到相对应的存储介质数据库里。然后就可以数据可视化展示出来，返回给用户看了。

离线处理流程
用户通过接口，做一些网页操作，以服务请求的形式，传到后台。后台会有一个记录，存到日志文件中，经过Flume日志采集之后，先把它存储起来，然后去做清洗，然后加载把它放到数仓里去，在数仓里，用hive，这样一些工具，把它做一些管理，接下来给到计算框架里，spark或者 tensorflow，这里边做完计算之后，最后，再存到相应的数据库里面去，最后给用户可视化展示。


2020年1月4日 星期六

数据源的解析

做推荐关键是要得到一个推荐列表，整体来讲，需要实时个性化推荐，离线的基于隐语义模型做的预测评分，和非个性化统计相关的推荐。所以就需要收集用户行为，比如点赞，记录下它的一些标签，根据点赞有相似推荐，然后是基于内容的推荐。

离线个性化推荐用到隐语义模型去做预测评分，弄一个离线推荐的服务，专门做这样一个评分。会用到 als，用 azkaban 做工作调度，把两部分离线服务（统计和预测评分）调度起来。设置一个定时任务，跑完之后，结果定时更新。得到的结果写到 mongo里。

实时推荐，首先是从业务后台或者其他到环境里收集日志，flume 拿到日志，然后会给到 kafka 做一个消息队列的缓冲。通过这样一个缓冲通道，把它可以给到 spark streaming 去做一个实时推荐。除了实时搜集来的行为数据，比如评分数据之外，还需要一些之前的相关数据，需要从redis里找，也有可能会到 mongo 和 es 里找，最后得到到结果还是写回到 mongo 里。




推荐模块的主要交互是和数据库进行，从数据库里面读数据，读完做分析，计算，得到的推荐结果，再写到对应的数据库里，后台业务把它读出来，展示给用户。离线个性化推荐用隐语义模型去做预测评分，会有一个离线服务专门做这样一个预测评分。用一个 azkaban 来做工作调度服务，把这两部分离线服务调度起来。定时跑这个任务，跑完之后，把结果定时更新结果写到 mongo里。

从业务后台收集日志，到 kafka 做一个消息队列缓冲通道，把它给他哦 spark streaming 去做一个实时推荐。除了实时收集来到行为数据，比如点赞、收藏数据，还需要一些之前的相关数据，除了从 redis 里找，还需要从 mongo 和 es 里拿。最后得到的结果，还要写回 mongo 里。


可视化给用户展示，检索服务、信息查询服务、标签服务、评分服务，这些服务都是业务已经实现好的。默认是有数据的，因此不存在冷启动。数据的部分涉及到两个部分，一部分往 mongo 里写数据，一部分往 es 里写数据，写入数据之后，就可以做离线统计服务了，这里可能涉及几个指标，比如说，句子点赞统计，句子收藏统计，类别的TopN。数据当从 mongo 里面读取出来之后，一个一个应该有一个推荐列表，最后在写到 mongo 里。这是统计模块的主要内容。离线推荐部分，主要得到的就是用户的推荐句子列表，也写到业务数据库里，另外还有一个算出来的副产品，电影直接的相似度，也写到对应 mongo 数据库里。实时推荐快的话，必须做一个缓存。用户做了一个点击，把这个请求发到后台，后台的句子点赞服务做一个响应，写到业务数据库，用flume 去采集，kafka 做消息队列缓冲，定义一个 filesource 和 kafkaSink，日志来了之后，先保存为文件，通过管道的通道给到 kafkasink ，kafka 这里可以定义不同的 topic 再去做一个过滤处理，做了日志提取之后，数据结构就把它中间用竖杠分割了几个字段，kafka 做完这样的处理之后，把内容给到实时推荐系统。这里的数据内容要过来，redis 里的数据也得过来，最后再把它写回到 mongo 里去。这就是整个实时推荐的一个过程。

句子的信息长什么样子，主要有几个字段？跟我们是否给他做推荐相关的 信息可能是那些？不会用id做推荐，句子长度会做推荐吗？关联度就不大，实现就找一个跟最后的推荐结果最相关的一个特征，做了特征选取。就是类别。认为它和推荐的内容最相关，接下来看评分信息数据很详尽，大量的工作是围绕评分数据和行为数据来做，隐语义模型就是基于它来做预测评分。最后做实时推荐的时候，电影的相似度也是基于这个来做的。还有电影标签信息，和评分数据非常接近，接下来要做的就是，从这些数据里面去做推荐，最详实的数据是电影信息评分数据。所以，主要基于这两部分数据。电影信息里最关键的是类别信息，所以基于内容推荐的时候，我们就用类别信息。另外还涉及到什么样的表呢？业务系统应该有一个用户表，这个不用去太关心。一开始去点选一个兴趣标签，那是一个冷启动，这个标签是直接和对应的电影类别匹配就可以了，业务系统直接就搞定了, 只要标签匹配，推出来就可以了，所以这部分我们就弱化了。我们关键要实现的推荐，需要有哪些表呢？统计推荐是每一个统计推荐都应该对应有一个表，一个离线用户统计的结果，一个实时推荐的结果，电影的相似度，在 mongo 里主要是这些内容。


从业务数据库 mongo 里读出来，读的过程中，就用 sparkSession里的 read方法，读出来就可以了，统计模型
核心就是基于 als的算法训练隐语义模型，训练出来的模型之后，就可以把两个特征矩阵，用户特征矩阵和电影特征矩阵，乘回去得到一个预测评分。根据预测评分，它的大小就可以得到用户的推荐矩阵。用户的推荐列表构成的一个矩阵，这就是我们的离线推荐要的内容。有一个副产品电影之间的相似度，已经拿到了电影的特征矩阵，每个电影的特征向量已知了，我们可以根据这个特征来求出它的相似度。基本的评分数据从评分表里 read 出来，转成一个 dataset 然后转成一个 rating 的数据结构，后面几个参数，一开始实现的话，就拍脑袋直接给一个完事儿，实际项目做参数调整，参数调整要做交叉验证，交叉验证得放到划分训练集测试集上面去做测试，用rmse来衡量。这个值越小，对应的参数越好。到时候，给一组参数，选一个均方根误差最小的参数，作为经典参数。模型的参数选择评估的过程，是一个分类问题，要想评价精确率和召回率，还得收集用户反馈数据。推荐系统最后你推荐出来的列表要去评价，还得用到这些。拿到训练的模型，要先做一个预测评分，传一个 uid mid 元组类型的 rdd，有一个空矩阵传进去，直接用每一个user和没一个movie做笛卡尔积，空矩阵传进去，就可以得到预测评分，把所有预测出来的电影的评分都聚合，得到一个评分的列表，列表里再去做一个排序，再读取里面的前多少个。就得到最后想要的用户推荐列表，一个用户对应一个 列表 seq。seq里面是一个电影对应一个评分。再把对应得到的结果写回到 mongo 里。这是离线推荐的结果。电影特征训练完成就得到了，这就是一个基本的思路。model.productfeaures 这就是model里定义好的一个属性，代表商品的特征， model训练好以后，有一个userfeatures 和 productfeatures，我们要的是 productfeatures ，得到这个电影的特征，要电影的相似度，要把两两的相似度都做一个计算，要把每一个电影和每一个电影做一个匹配，好像是做一个全外连接，把它算一个笛卡尔积，得到的结果就相当于是每一个电影和每一个电影对应上作为一个元素，特征向量都已经知道了，根据余弦相似度，求相似度，这个余弦值就可以作为相似度存储起来，大家可以看到整体的流程， 算出来的内容，一部分要存到 mongo 里，评分矩阵，一部分实时推荐系统要从缓存里拿，要把它存到 redis 里面。实时推荐模块是整个框架的这一小块儿，实时推荐，要求就是要速度快，基于这样的要求，结果可以不那么精确，可以提前把相似度算好，不需要实时去更新，然后要有一个预先设计好的模型，根据这个模型，也就是预定的规则，接一算，就把它算出来这样就很快。要做的计算只是针对当前这个用户这次评分，相关的一些计算，不要计算所有的。整体流程就是，来的数据就是一个用户评分数据，收集到日志里面来，要过 flume 然后 kafka ，最后输入到 spark streaming 实时推荐算法来，streaming 实时推荐算法还是要用到 redis 里面的数据的，最后的结果写回到 mongo 里去。
